# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cnx17I-dgy3fvTZvL3D9BTowI4dz1u5e
"""

#Data downloading and merging

import pandas as pd

# Load the first CSV file with 'id' and 'text' columns
df_text = pd.read_csv('./Data/reviews.csv')

# Load the second CSV file with 'id' and 'sentiment' columns
df_sentiment = pd.read_csv('./Data/labels.csv')

# Merge the two DataFrames on the 'id' column to combine the information
merged_df = df_text.merge(df_sentiment, on='id')

# Convert 'Positive' to 1 and 'Negative' to 0
merged_df['sentiment'] = merged_df['sentiment'].map({'Positive': 1, 'Negative': 0})

# Convert the 'sentiment' column to 'int32'
merged_df['sentiment'] = merged_df['sentiment'].astype('int32')

# Create a DataFrame with only 'text' and 'sentiment' columns
selected_columns_df = merged_df[['text', 'sentiment']]

# Save the selected DataFrame as a common CSV file
selected_columns_df.to_csv('./Data/selected_data.csv', index=False)

# Save the selected DataFrame as a JSON file
selected_columns_df.to_json('./Data/selected_data.json', orient='records')

import pandas as pd
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.optim import AdamW

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score

# Load and preprocess your dataset
df = pd.read_csv('./Data/selected_data.csv')

# Split your dataset into training and validation sets
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

# Define your model and tokenizer
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Assuming binary classification

# Define your optimizer and learning rate
optimizer = AdamW(model.parameters(), lr=1e-5)

# Custom Dataset class to handle tokenization and formatting
class SentimentDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = str(self.data.iloc[idx]['text'])
        label = int(self.data.iloc[idx]['sentiment'])
        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')
        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Create data loaders
batch_size = 8
train_dataset = SentimentDataset(train_df, tokenizer)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataset = SentimentDataset(val_df, tokenizer)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size)

# Training loop
num_epochs = 6
device = torch.device("cpu")
model.to(device)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch in train_dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    average_loss = total_loss / len(train_dataloader)
    print(f"Epoch {epoch + 1} - Average Loss: {average_loss}")

# Evaluation loop
model.eval()
predictions = []
true_labels = []

with torch.no_grad():
    for batch in val_dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()

        predictions.extend(predicted_labels)
        true_labels.extend(labels.cpu().numpy())

# Calculate evaluation metrics
accuracy = accuracy_score(true_labels, predictions)
f1 = f1_score(true_labels, predictions, average='weighted')  # You can choose the appropriate averaging strategy

print(f"Accuracy: {accuracy}")
print(f"F1 Score: {f1}")

# Save only the model's trained parameters
torch.save(model.state_dict(), "./model/saved_model.pth")

# Save the model to a directory
model.save_pretrained("./model")

# Save the tokenizer associated with the model
tokenizer.save_pretrained("./model")